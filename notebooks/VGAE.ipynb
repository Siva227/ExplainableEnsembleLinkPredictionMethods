{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pritishmishra/opt/anaconda3/envs/torch_gpu_arm64/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch_geometric'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch_geometric\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mimport\u001b[39;00m GCNConv\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mF\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mEncoder\u001b[39;00m(torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch_geometric'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, 2 * out_channels)\n",
    "        self.conv_mu = GCNConv(2 * out_channels, out_channels)\n",
    "        self.conv_logvar = GCNConv(2 * out_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        return self.conv_mu(x, edge_index), self.conv_logvar(x, edge_index)\n",
    "\n",
    "\n",
    "class VGAE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(VGAE, self).__init__()\n",
    "        self.encoder = Encoder(in_channels, out_channels)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            return mu + torch.randn_like(logvar) * torch.exp(logvar)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        mu, logvar = self.encoder(x, edge_index)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return z, mu, logvar\n",
    "\n",
    "\n",
    "def loss_function(pos_pred, neg_pred, mu, logvar):\n",
    "    recon_loss = -torch.mean(torch.log(pos_pred + 1e-15) + torch.log(1 - neg_pred + 1e-15))\n",
    "    KLD_loss = -0.5 / mu.size(0) * torch.mean(\n",
    "        torch.sum(1 + 2 * logvar - mu.pow(2) - logvar.exp().pow(2), dim=1)\n",
    "    )\n",
    "    return recon_loss + KLD_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl.nn as dglnn\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = dglnn.GraphConv(in_feats, 2 * out_feats, activation=F.relu)\n",
    "        self.conv2 = dglnn.GraphConv(2 * out_feats, 2 * out_feats)  # output 2 * out_feats\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        h = self.conv1(g, features)\n",
    "        h = self.conv2(g, h)\n",
    "        mu, logvar = torch.chunk(h, 2, dim=1)  # Split the output into mu and logvar\n",
    "        return mu, logvar\n",
    "\n",
    "class VGAE(nn.Module):\n",
    "    def __init__(self, g, in_feats, latent_feats):\n",
    "        super(VGAE, self).__init__()\n",
    "        self.g = g\n",
    "        self.encoder = Encoder(in_feats, latent_feats)\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = torch.exp(logvar).pow(0.5)\n",
    "            eps = torch.randn_like(std)\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def forward(self, features):\n",
    "        mu, logvar = self.encoder(self.g, features)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return z, mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, graph, features, epochs=200):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # ensure the model is in training mode\n",
    "        z, mu, logvar = model(features)\n",
    "        \n",
    "        # Compute the reconstruction loss and the KL divergence\n",
    "        recons_loss = F.binary_cross_entropy_with_logits(model.decode(z)[graph.edge_index[0], graph.edge_index[1]], torch.ones(graph.num_edges()).to(device))\n",
    "        kl_div = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "        loss = recons_loss + kl_div\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print('Epoch:', epoch+1, ', Loss:', loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(model, graph, features):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():  # No need to track gradients\n",
    "        z, _, _ = model(graph, features)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NumNodes: 2708\n",
      "  NumEdges: 10556\n",
      "  NumFeats: 1433\n",
      "  NumClasses: 7\n",
      "  NumTrainingSamples: 140\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pritishmishra/opt/anaconda3/envs/torch_gpu_arm64/lib/python3.9/site-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Cannot find DGL C++ sparse library at /Users/pritishmishra/opt/anaconda3/envs/torch_gpu_arm64/lib/python3.9/site-packages/dgl/dgl_sparse/libdgl_sparse_pytorch_1.14.0.dev20221204.dylib",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m z, mu, logvar \u001b[39m=\u001b[39m model(features)\n\u001b[1;32m     35\u001b[0m \u001b[39m# Compute adjacency matrix\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m adj \u001b[39m=\u001b[39m g\u001b[39m.\u001b[39;49madjacency_matrix()\u001b[39m.\u001b[39mto_dense()\n\u001b[1;32m     38\u001b[0m \u001b[39m# Compute the reconstruction loss and the KL divergence\u001b[39;00m\n\u001b[1;32m     39\u001b[0m recons_loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mbinary_cross_entropy_with_logits(model\u001b[39m.\u001b[39mdecode(z), adj)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch_gpu_arm64/lib/python3.9/site-packages/dgl/heterograph.py:3759\u001b[0m, in \u001b[0;36mDGLGraph.adjacency_matrix\u001b[0;34m(self, etype)\u001b[0m\n\u001b[1;32m   3757\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madjacency_matrix\u001b[39m(\u001b[39mself\u001b[39m, etype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   3758\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Alias of :meth:`adj`\"\"\"\u001b[39;00m\n\u001b[0;32m-> 3759\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madj(etype)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch_gpu_arm64/lib/python3.9/site-packages/dgl/heterograph.py:3821\u001b[0m, in \u001b[0;36mDGLGraph.adj\u001b[0;34m(self, etype, eweight_name)\u001b[0m\n\u001b[1;32m   3818\u001b[0m \u001b[39m# Temporal fix to introduce a dependency on torch\u001b[39;00m\n\u001b[1;32m   3819\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m-> 3821\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39msparse\u001b[39;00m \u001b[39mimport\u001b[39;00m spmatrix\n\u001b[1;32m   3823\u001b[0m etype \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_canonical_etype(etype)\n\u001b[1;32m   3824\u001b[0m indices \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_edges(etype\u001b[39m=\u001b[39metype))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch_gpu_arm64/lib/python3.9/site-packages/dgl/sparse/__init__.py:43\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:  \u001b[39m# pylint: disable=W0703\u001b[39;00m\n\u001b[1;32m     40\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCannot load DGL C++ sparse library\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 43\u001b[0m load_dgl_sparse()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch_gpu_arm64/lib/python3.9/site-packages/dgl/sparse/__init__.py:35\u001b[0m, in \u001b[0;36mload_dgl_sparse\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(dirname, \u001b[39m\"\u001b[39m\u001b[39mdgl_sparse\u001b[39m\u001b[39m\"\u001b[39m, basename)\n\u001b[1;32m     34\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(path):\n\u001b[0;32m---> 35\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCannot find DGL C++ sparse library at \u001b[39m\u001b[39m{\u001b[39;00mpath\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     37\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m     torch\u001b[39m.\u001b[39mclasses\u001b[39m.\u001b[39mload_library(path)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Cannot find DGL C++ sparse library at /Users/pritishmishra/opt/anaconda3/envs/torch_gpu_arm64/lib/python3.9/site-packages/dgl/dgl_sparse/libdgl_sparse_pytorch_1.14.0.dev20221204.dylib"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "from dgl.data import CoraGraphDataset\n",
    "\n",
    "# Load the Cora dataset\n",
    "data = CoraGraphDataset()\n",
    "g = data[0]\n",
    "\n",
    "# Prepare the input features and labels\n",
    "features = g.ndata['feat']\n",
    "labels = g.ndata['label']\n",
    "\n",
    "# Normalize the features (this is often a good preprocessing step)\n",
    "features = features / features.norm(dim=1, keepdim=True)\n",
    "\n",
    "# Initialize the model with the appropriate dimensions\n",
    "# Cora has 1433 input features, and let's use 50 for the latent space dimension\n",
    "model = VGAE(g, 1433, 50)\n",
    "\n",
    "# Move everything to the GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "g = g.to(device)\n",
    "features = features.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "# Train the model\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "epochs = 200\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # ensure the model is in training mode\n",
    "    z, mu, logvar = model(features)\n",
    "\n",
    "    # Compute adjacency matrix\n",
    "    adj = g.adjacency_matrix().to_dense()\n",
    "\n",
    "    # Compute the reconstruction loss and the KL divergence\n",
    "    recons_loss = F.binary_cross_entropy_with_logits(model.decode(z), adj)\n",
    "    kl_div = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    loss = recons_loss + kl_div\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print('Epoch:', epoch+1, ', Loss:', loss.item())\n",
    "\n",
    "# Generate node embeddings\n",
    "embeddings = model(features)[0].detach().cpu().numpy()  # get the latent representations and move to cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.14.0.dev20221204'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'EXPLAINABLEENSEMBLELINKPREDICTION'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mF\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mEXPLAINABLEENSEMBLELINKPREDICTION\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39meelp\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39margs\u001b[39;00m \u001b[39m# Import arguments from args.py\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'EXPLAINABLEENSEMBLELINKPREDICTION'"
     ]
    }
   ],
   "source": [
    "# Module imports\n",
    "import os\n",
    "import torch\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import EXPLAINABLEENSEMBLELINKPREDICTION.eelp.utils.args # Import arguments from args.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining global helper functions\n",
    "def dot_product_decode(Z):\n",
    "    A_pred = torch.sigmoid(torch.matmul(Z, Z.t()))\n",
    "    return A_pred\n",
    "\n",
    "def glorot_init(input_dim, output_dim):\n",
    "    init_range = np.sqrt(6.0 / (input_dim + output_dim))\n",
    "    initial = torch.rand(input_dim, output_dim) * 2 * init_range - init_range\n",
    "    return nn.Parameter(initial)\n",
    "\n",
    "# Defining the model\n",
    "class VGAE(nn.Module):\n",
    "    # Defining the constructor\n",
    "    def __init__(self, adj):\n",
    "        super(VGAE, self).__init__()\n",
    "        self.base_gcn = GraphConvSparse(args.input_dim, args.hidden1_dim, adj)\n",
    "        self.gcn_mean = GraphConvSparse(args.hidden1_dim, args.hidden2_dim, adj, activation=lambda x: x)\n",
    "        self.gcn_logstddev = GraphConvSparse(args.hidden1_dim, args.hidden2_dim, adj, activation=lambda x: x)\n",
    "\n",
    "    # Defining the encoder\n",
    "    def encode(self, X):\n",
    "        hidden = self.base_gcn(X)\n",
    "        self.mean = self.gcn_mean(hidden)\n",
    "        self.logstd = self.gcn_logstddev(hidden)\n",
    "        gaussian_noise = torch.randn(X.size(0), args.hidden2_dim)  # Gaussian noise\n",
    "        sampled_z = gaussian_noise * torch.exp(self.logstd) + self.mean  # Reparameterization trick\n",
    "        return sampled_z\n",
    "    \n",
    "    # Defining the forward layer\n",
    "    def forward(self, X):\n",
    "        z = self.encode(X)\n",
    "        return z\n",
    "    \n",
    "class GraphConvSparse(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, adj, activation=F.relu, **kwargs):\n",
    "        super(GraphConvSparse, self).__init__(**kwargs)\n",
    "        self.weight = glorot_init(input_dim, output_dim)  # Weight initialization\n",
    "        self.adj = adj  # Adjacency matrix\n",
    "        self.activation = activation  # Activation function\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        x = torch.mm(x, self.weight)  # Matrix multiplication\n",
    "        x = torch.mm(self.adj, x)  # Convolution operation\n",
    "        outputs = self.activation(x)  # Activation\n",
    "        return outputs\n",
    "    \n",
    "class GAE(nn.Module):\n",
    "    def __init__(self, adj):\n",
    "        super(GAE, self).__init__()\n",
    "        self.base_gcn = GraphConvSparse(args.input_dim, args.hidden1_dim, adj)\n",
    "        self.gcn_mean = GraphConvSparse(args.hidden1_dim, args.hidden2_dim, adj, activation=lambda x: x)\n",
    "\n",
    "    def encode(self, X):\n",
    "        hidden = self.base_gcn(X)\n",
    "        z = self.mean = self.gcn.mean(hidden)\n",
    "        return z\n",
    "    \n",
    "    def forward(self, X):\n",
    "        z = self.encode(X)\n",
    "        A_pred = dot_product_decode(z)\n",
    "        return A_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'args'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39margs\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'args'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu_arm64",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
